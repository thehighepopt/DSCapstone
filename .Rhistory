install.packages("caret")
library(kernlab)
library(caret)
brats <- read.csv('C:/Users/stephen.p.duffy/Documents/GitHub/New Folder/Brats.csv')
brats$material <- as.character(brats$material)
brats$amount <- as.numeric(brats$amount)
spices <- data.frame(tally(group_by(brats,material),sort = TRUE ))
names(spices) <- c("material","count")
brats$grams <- ifelse(brats[,3] == "tsp", brats$grams <- brats$amount * 4.93,
ifelse(brats[,3] == "tbsp", brats$grams <- brats$amount * 15,
ifelse(brats[,3] == "lbs", brats$grams <- brats$amount * 453.6,
ifelse(brats[,3] == "g", brats$grams <- brats$amount * 1, NA))))
brats[is.na(brats)] <- 0
bymat <- brats %>% group_by(material) %>% summarise_each(funs(mean),grams)
spices <- join(spices,bymat,by="material")
View(spices)
View(bymat)
library(kernlab)
data(spam)
data("faithful")
inTrain <- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
install.packages("ILSR")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
View(predictors)
data("concrete")
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(concrete)
qplot(CompressiveStrength.,finMod$residuals, colour=race,data=training)
View(testing)
View(training)
qplot(CompressiveStrength ~ .,colour=.,data=concrete)
qplot(CompressiveStrength ~ .,colour=concrete[,1:8],data=concrete)
featurePlot(x=training[,1:8],
y=training$CompressiveStrength,
plot="pairs")
librar(Hmisc)
library(Hmisc)
featurePlot(x=concrete[,1:8],
y=concrete$CompressiveStrength,
plot="pairs")
conc2 <- concrete
summary(conc2$FlyAsh)
conc2$FlyAsh <- cut2(conc2$FlyAsh,c(25,50,75,100,125,150,175,200))
summary(conc2$Age)
conc2$FlyAsh <- cut2(conc2$FlyAsh,c(50,100,150,200,250,300))
conc2$Age <- cut2(conc2$Age,c(50,100,150,200,250,300))
featurePlot(x=conc2[,1:8],
y=conc2$CompressiveStrength,
plot="pairs")
View(conc2)
summary(conc2)
conc2$Water <- cut2(conc2$Water,c(50,100,150,200,250,300))
conc2$BlastFurnaceSlag <- cut2(conc2$BlastFurnaceSlag,c(50,100,150,200,250,300))
conc2$CourseAggregate <- cut2(conc2$CourseAggregate,c(100,200,300,400,500,600,700,800,900))
conc2$CoarseAggregate <- cut2(conc2$CoarseAggregate,c(100,200,300,400,500,600,700,800,900))
conc2$FineAggregate <- cut2(conc2$FineAggregate,c(100,200,300,400,500,600,700,800,900))
conc2$Superplasticizer <- cut2(conc2$Superplasticizer,c(5,10,15,20,25,30))
conc2$Cement <- cut2(conc2$Cement,c(100,200,300,400,500))
featurePlot(x=conc2[,1:8],
y=conc2$CompressiveStrength,
plot="pairs")
featurePlot(x=training[,1:8],
y=training$CompressiveStrength,
plot="pairs")
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index,y=CompressiveStrength)) + geom_point() + theme_bw()
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
View(mixtures)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
featurePlot(x = training[, names], y = cutCS, plot = "box")
featurePlot(x = training[, 1:8], y = cutCS, plot = "box")
hist(training$Superplasticizer)
View(mixtures)
summary(mixtures$Superplasticizer)
summary(cement$Superplasticizer)
summary(concrete$Superplasticizer)
hist(log(mixtures$Superplasticizer))
hist(log10(mixtures$Superplasticizer))
summary(log10(cement$Superplasticizer))
summary(log10(concrete$Superplasticizer))
g <- log10(concrete$Superplasticizer)
plot(g)
hist(g)
summary(g)
g <- log(concrete$Superplasticizer)
plot(g)
hist(g)
g <- log(mixtures$Superplasticizer)
plot(g)
hist(g)
summary(g)
g <- log(mixtures$Superplasticizer+1)
hist(g)
g <- log(training$Superplasticizer)
hist(g)
summary(g)
hist(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]];training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
match("IL",names(training))
lab <- glob2rx("IL*")
grep(lab,training)
lab <- glob2rx("IL_*")
grep(lab,training)
grep(lab,training,value = TRUE)
grep(lab,names(training))
grep(glob2rx("IL*"),names(training))
cols <- grep(glob2rx("IL*"),names(training))
b <- training[,cols]
View(b)
g <- preProcess(b,thresh = .80)
g
g[1]
g[2]
g[3]
g[4]
g[5]
g[6]
g[7]
g[8]
g[9]
g[10]
g[11]
g[12]
g[13]
g[14]
g <- preProcess(b,method = "pca", thresh = .80)
g[4]
traing <- predict(g,b)
View(traing)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]];training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]];training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
cols <- grep(glob2rx("IL*"),names(training))
b <- training[,c(1,cols)]
View(b)
pred1 <- glm(diagnosis ~ ., data = training)
View(training)
pred1 <- glm(diagnosis ~ ., data = b)
View(b)
b[,2:13] <- as.numeric(b[,2:13])
b[,2] <- as.numeric(b[,2])
b[,3:4] <- as.numeric(b[,3:4])
sapply(b, as.numeric(b))
b[,3] <- as.numeric(b[,3])
b[,4] <- as.numeric(b[,4])
b[,5] <- as.numeric(b[,5])
b[,6] <- as.numeric(b[,6])
b[,7] <- as.numeric(b[,7])
b[,8] <- as.numeric(b[,8])
b[,9] <- as.numeric(b[,9])
b[,10] <- as.numeric(b[,10])
b[,11] <- as.numeric(b[,11])
b[,12] <- as.numeric(b[,12])
b[,13] <- as.numeric(b[,13])
pred1 <- glm(diagnosis ~ ., data = b)
str(b)
b[,1] <- as.character(b[,1])
pred1 <- glm(diagnosis ~ ., data = b)
View(b)
pred1 <- lm(diagnosis ~ ., data = b)
cols <- grep(glob2rx("IL*"),names(training))
b <- training[,c(1,cols)]
b[,3] <- as.numeric(b[,3])
b[,4] <- as.numeric(b[,4])
b[,5] <- as.numeric(b[,5])
b[,6] <- as.numeric(b[,6])
b[,7] <- as.numeric(b[,7])
b[,8] <- as.numeric(b[,8])
b[,9] <- as.numeric(b[,9])
b[,10] <- as.numeric(b[,10])
b[,11] <- as.numeric(b[,11])
b[,12] <- as.numeric(b[,12])
b[,13] <- as.numeric(b[,13])
pred1 <- glm(diagnosis ~ ., data = b)
pred1 <- lm(diagnosis ~ ., data = b)
summary(pred1)
View(b)
str(b)
b[,1] <- as.factor(b[,1])
str(b)
pred1 <- lm(diagnosis ~ ., data = b)
pred1 <- train(diagnosis ~ .,method ="glm", data=b)
install.packages('e1071', dependencies=TRUE)
pred1 <- train(diagnosis ~ .,method ="glm", data=b)
summary(pred1)
C1 <- confusionMatrix(predictions, testing$diagnosis)
C1 <- confusionMatrix(b, pred1$diagnosis)
predictions <- predict(modelFit, newdata = testing)
predictions <- predict(pred1, newdata = b)
C1 <- confusionMatrix(predictions, pred1$diagnosis)
C1 <- confusionMatrix(predictions, b$diagnosis)
c1
C1
A1 <- C1$overall[1]
A1
pred2 <- train(diagnosis ~ .,method ="glm",preProcess = "pca", data=b)
predictions2 <- predict(pred2, newdata = b)
C2 <- confusionMatrix(predictions2, b$diagnosis)
A1 <- C2$overall[1]
A1 <- C1$overall[1]
A2 <- C2$overall[1]
A1;A2
pred2 <- train(diagnosis ~ .,method ="glm",preProcess = "pca", data=b, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
predictions2 <- predict(pred2, newdata = b)
C2 <- confusionMatrix(predictions2, b$diagnosis)
A2 <- C2$overall[1]
A1;A2
predictions2 <- predict(pred2, newdata = testing)
C2 <- confusionMatrix(predictions2, testing$diagnosis)
A2 <- C2$overall[1]
pred1 <- train(diagnosis ~ .,method ="glm", data=b)
C1 <- confusionMatrix(predictions, testing$diagnosis)
C1 <- confusionMatrix(pred1, testing$diagnosis)
predictions <- predict(pred1, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
A1 <- C1$overall[1]
A1;A2
cols <- grep(glob2rx("IL*"),names(training))
b <- training[,cols]
g <- preProcess(b,method = "pca", thresh = .90)
traing <- predict(g,b)
traing
View(traing)
install_github('ramnathv/rCharts')
library(devtools)
install_github('ramnathv/rCharts')
library(rCharts)
hair_eye = as.data.frame(HairEyeColor)
rPlot(Freq ~ Hair | Eye, color = 'Eye', data = hair_eye, type = 'bar')
haireye <- as.data.frame(HairEyeColor)
n1 <- nPlot(Freq ~ Hair, group = 'Eye', type = 'multiBarChart', data = subset(haireye, Sex == 'Male'))
nPlot(Freq ~ Hair, group = 'Eye', type = 'multiBarChart', data = subset(haireye, Sex == 'Male'))
n1 <- nPlot(Freq ~ Hair, group = 'Eye', type = 'multiBarChart', data = subset(haireye, Sex == 'Male'))
install.packages('slidify')
n1 <- nPlot(Freq ~ Hair, group = 'Eye', type = 'multiBarChart', data = subset(haireye, Sex == 'Male'))
n1$save('fig/n1.html',cdn =TRUE)
cat('<iframe src="fig/n1/html" width=100%, height=600></iframe>')
names(iris) = gsub("\\.", "", names(iris))
rPlot(SepalLength ~ SepalWidth | Species, data = iris, color = 'Species', type = 'point')
install.packages('morris')
library(leaflet)
library(highchart)
install.packages('highchart')
install.packages('googleviz')
install_github("slidify/ramnathv")
install_github("slidify", "ramnathv")
install_github("ramnathv/slidify")
library(slidify)
install.packages('googleVis')
suppressPackageStartupMessages(library(googleVis))
M <- gvisMotionChart(Fruits,"Fruit","Year",options = list(width = 600, height = 400))
print(M,"chart")
plot(M)
G <- gvisGeoChart(Exports, locationvar = "Country", colorvar = "Profit", options = list(width = 600,
height = 400))
plot(G)
G <- gvisGeoChart(Exports, locationvar = "Country", colorvar = "Profit", options = list(width = 600,
height = 400, region = "150"))
plot(G)
demo(googleVis)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
plot(myplot)
plot(myPlot)
myPlot(s)
myPlot(5)
myPlot(1.3)
myPlot(.3)
library(rsconnect)
library(shiny)
library(shinyapps)
library(devtools)
library(ggplot2)
library(rCharts)
dTable(airquality, sPaginationType = "full_numbers")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h2('Sidebar')
),
mainPanel(
h2('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text') ,
h3('Sidebar')),
mainPanel(
h3('Main Panel text')
)
))
shiny::runApp('GitHub/DataProducts')
getwd()
setwd("C:/Users/stephen.p.duffy/Documents/GitHub/DSCapstone")
library(tm)
library(plyr)
f <- DirSource("./Coursera-Swiftkey/final/en_US", encoding = "en_US",ignore.case = TRUE,mode = "text")
o <- data.frame(String=character())
View(o)
View(o)
for (i in 1:3) {
con <- file(f$filelist[i],"r")
g <- as.data.frame(readLines(con,3000000))
names(g) <- c("String")
o <- rbind(g,o)
rm(g)
close(con)
}
set.seed(2317)
ins <- rbinom(nrow(o),1,.05)
d <- cbind(o,as.data.frame(ins))
test <- subset(o,ins == 1)
View(test)
rm(d,o,con,i,ins,f)
f <- DirSource("./Coursera-Swiftkey/final/en_US", encoding = "en_US",ignore.case = TRUE,mode = "text")
o <- data.frame(String=character())
for (i in 1:3) {
con <- file(f$filelist[i],"r")
g <- as.data.frame(readLines(con,3000000))
names(g) <- c("String")
o <- rbind(g,o)
rm(g)
close(con)
}
f
blogs <- as.data.frame(readLines(file(f$filelist[1],"r"),1000000))
news <- as.data.frame(readLines(file(f$filelist[2],"r"),2000000))
twitter <- as.data.frame(readLines(file(f$filelist[3],"r"),3000000))
news$len <- length(news[,1])
View(news)
news <- news[,1]
news <- as.data.frame(readLines(file(f$filelist[2],"r"),2000000))
news$len <- nchar(news[,1])
View(news)
str(news)
news$len <- nchar(as.character(news[,1]))
max(news[,2])
blogs$len <- nchar(as.character(blogs[,1]))
max(blogs[,2])
l <- subset(twitter,twitter[,1]== "[Ll]ove" )
l <- subset(twitter,twitter[,1]== "\.*[Ll]ove\.*" )
l <- subset(twitter,twitter[,1]== ".*[Ll]ove.*" )
l <- subset(twitter,twitter[,1]== .*[Ll]ove.* )
l <- subset(twitter,grepl(".*[Ll]ove.*",twitter))
l <- subset(twitter,grepl("[Ll]ove",twitter))
l <- subset(twitter,grepl("Love",twitter,ignore.case = TRUE))
l <- subset(twitter,grepl("[Love]",twitter,ignore.case = TRUE))
l <- subset(twitter,grepl(".*[Ll]ove.*",twitter))
View(l)
l <- subset(twitter,grep("[Ll]ove",twitter))
l <- tm_filter(twitter, FUN = function(x) any(grep("[Ll]ove", content(x))))
l <- subset(twitter,grepl("love",twitter))
rm(l)
l <- grep("love",twitter)
l <- grep("love",twitter[,1])
l <- subset(twitter,grepl("[Ll]ove",twitter[,1]))
p <- subset(twitter,grepl("[Hh]ate",twitter[,1]))
nrow(l)/nrow(p)
p <- subset(twitter,grepl("biostats",twitter[,1]))
View(p)
p
p <- subset(twitter,grepl("A computer once beat me at chess, but it was no match for me at kickboxing",twitter[,1]))
p
rm(p,l)
getwd()
setwd("C:/Users/stephen.p.duffy/Documents/GitHub")
elpaso <- read.csv("Book2.csv")
View(elpaso)
library(fma)
library(ets)
library(forecast)
View(elpaso)
f <- ets(elpaso[,3])
f
f$aic
f$loglik
myts <- ts(elpaso, start=c(2014, 2), end=c(2016, 7), frequency=12)
myts <- ts(elpaso[,3], start=c(2014, 2), end=c(2016, 7), frequency=12)
f <- ets(myts)
f
f$fit
f
f$states
f$x
f$m
f$components
f <- tslm(myts)
f <- tslm(myts ~ trend + season)
f
plot(f$coefficients)
plot(f$coefficients,type = "l")
f <- ets(myts)
f$components
f$residuals
plot(stl(myts,s.window = "periodic"))
j <- stl(mytx,s.window = "periodic")
j <- stl(myts,s.window = "periodic")
j$time.series
j <- stl(elpaso[,3],s.window = "periodic")
myts
elpaso[,3]
myts <- ts(elpaso[,3], start=c(2014, 2), end=c(2016, 7), frequency=12)
myts
f <- ets(myts)
f
f <- ets(myts,"A,N,A")
f <- ets(myts,"ANA")
f
f <- ets(myts)
View(o)
p <- as.VCorpus(o)
p <- getElem(o)
p <- pGetElem(o)
f <- DirSource("./Coursera-Swiftkey/final/en_US", encoding = "en_US",ignore.case = TRUE,mode = "text")
getwd()
setwd("C:/Users/stephen.p.duffy/Documents/GitHub/DSCapstone")
f <- DirSource("./Coursera-Swiftkey/final/en_US", encoding = "en_US",ignore.case = TRUE,mode = "text")
f$filelist
p <- VCorpus(f)
t <- Corpus(DirSource("./Coursera-Swiftkey/final/en_US", encoding = "en_US",ignore.case = TRUE,mode = "text"))
t <- Corpus(DirSource("./Coursera-Swiftkey/final/en_US", encoding = "en_US",ignore.case = TRUE,mode = "text"),readerControl = list(reader=readPlain))
t <- Corpus(DirSource("./Coursera-Swiftkey/final/en_US", ignore.case = TRUE,mode = "text"),readerControl = list(reader=readPlain))
e <- Corpus(dir)
rm(o,news,twitter,blogs,i,j,con)
dir <- DirSource("./Coursera-Swiftkey/final/en_US", ignore.case = TRUE,mode = "text")
e <- Corpus(dir)
rm(e,t)
corpus <- Corpus(dir)
blog <- corpus$en_US.blogs.txt
corpus$en_US.blogs.txt
corpus[1]
corpus[2]
corpus[3]
corpus[4]
